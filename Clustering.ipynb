{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd1HuBmuKqBM"
      },
      "source": [
        "**Event Detection in Social Media Streams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vDaX1Xo5f_eC",
        "outputId": "0ccd0fe1-5186-4490-88ae-36b42712a1d2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') # downloading the stopwords corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ONDVTqaouGDO",
        "outputId": "9bec3646-bebe-4c77-923c-8b92f936ff08"
      },
      "outputs": [],
      "source": [
        "!pip install google-api-python-client pandas scikit-learn nltk wordcloud matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlYxiTOvKcKG"
      },
      "outputs": [],
      "source": [
        "#-----------------------step 1: importing libraries---------------------------\n",
        "import pandas as pd # for data handling\n",
        "import numpy as np # for numerical\n",
        "import re # for matching patterns, searching and replacing. can group the parts of a pattern.\n",
        "import matplotlib.pyplot as plt  # for visualization\n",
        "import seaborn as sns  # for better visualization\n",
        "from wordcloud import WordCloud # if the frequesncy of a word is greater than it will be representated in greater size. more the frequency greater the size\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # TfidfVectorizer handles tokenizaton, lowercasing, removing stopwords. alos coverts to arrays\n",
        "from sklearn.cluster import KMeans # to form clusters or say clustering tweets into events\n",
        "import nltk  # natural language processing. has many features some are: remove the prefix and suffix from sentences and still keep its meaning, categorize words under noun, adjectives, verb,etc, help identify the words such as if the word is a location, thing, person\n",
        "from nltk.tokenize import word_tokenize # breaking sentences into words\n",
        "from nltk.corpus import stopwords # remove the irrelevent words such as if, and, are, etc of different language\n",
        "from collections import Counter # helps in counting the words in a\n",
        "from scipy.cluster.hierarchy import linkage, fcluster # linkage is used to perform hierarchical clustering.it finds the distance between the d\n",
        "from scipy.cluster.hierarchy import linkage,fcluster,dendrogram # linkage is used for getting the klinkage matrix that tell us about the clusters. linkage matrix consist of: list of clusters being murged, distance between clusters, how many new data points are now part of the new cluster after the merge. fcluster is used to make a get: like maximum number of clusters,  how clusters are formed and represented after linkage.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VUHQZwp1gjRA",
        "outputId": "168d5ca5-12fc-4d05-dedf-047f664bf747"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/GBcomments.csv', on_bad_lines='skip', quoting=csv.QUOTE_MINIMAL,engine='python')\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "df.rename(columns={'comment_text':'Comment'}, inplace=True)\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xldozb1bGo2L",
        "outputId": "06c4a4fe-fa6b-4a9c-8f6e-9eac0a4ad4df"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords') # downloading stopwords\n",
        "nltk.download('punkt_tab') # punkt is used to convert text into sentances and than words\n",
        "def preprocess_text(text):\n",
        "  text=str(text).lower() # coverting all the text in the comment to lowercase\n",
        "  text=re.sub(r\"https?://\\S+\",'',text) # re.sub is used to search a patter. removing the texts. the r represents the the data is raw which means that \\ are used as literal characters not the escape characters.http part matches the literal string and s? after that means that s is optional which helps to match with both http:// and https:// . now,  :// also matches the literal characters. than again, \\s matches the any non-whitespace  characters. the + is with \\s which gives \\s+ means typically the rest of the url\n",
        "  text=re.sub(r\"\\W\", ' ', text) # removing the speacial characters. \\W matches any character that is not a word. it includes uppercase, lowercase, digits, u=underscore. it does not involve spaces,punctuation marks, speacial characters, any other character that won't be aphanumeric or underscore. we provided a blank space in ' ' becuse when we remove the punchuations and replace them with a blank space than it helps from getting the text from being corelated to each other\n",
        "  text=re.sub(r\"\\s+\",' ', text) # removing the extra spaces\n",
        "  words=word_tokenize(text) # tokenizing. this involves  splitting text into individual words.\n",
        "  words=[ word for word in words if word not in stopwords.words('english')] #Remove stopwords. stopwords(is, in, and, etc) carry very little meaning.\n",
        "  return' '.join(words)\n",
        "df[\"Cleaned_Comment\"]=df['Comment'].apply(preprocess_text) # calling the function to clean the Comments\n",
        "print(df[[\"Comment\", \"Cleaned_Comment\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1mRBz3hyX2C9",
        "outputId": "d5a79200-3c92-4679-e639-1df26ddbaa08"
      },
      "outputs": [],
      "source": [
        "# now we need to convert texts into numbers( TF-IDF vectorization)\n",
        "vectorizer=TfidfVectorizer(max_features=1000) # we are converting the texts into the matrixes to find how imaportant, rare the word is. the matrix consists of the corresponding TF-IDF score\n",
        "x= vectorizer.fit_transform(df['Cleaned_Comment']) # converting the text data into numarical values\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mu-4TWLsYZV-",
        "outputId": "f1897992-bf25-45d9-91ad-c9c616575419"
      },
      "outputs": [],
      "source": [
        "# we will be applying k mean clustering\n",
        "number_of_clusters=5 # limiting the number of clusters i want\n",
        "kmeans=KMeans(n_clusters=number_of_clusters, random_state=42)\n",
        "df['KMeans_Cluster']=kmeans.fit_predict(x)\n",
        "print(df[[\"Comment\", \"KMeans_Cluster\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "zANqMeuuY0Nu",
        "outputId": "fb9d8303-e0b7-4bcc-8b4c-75d6c1a2ddbd"
      },
      "outputs": [],
      "source": [
        "# we will be Visualize Clusters with Word Clouds. a word cloud is a visual representation of text data based on the word size and its frequencies\n",
        "for cluster in range(number_of_clusters):\n",
        "  cluster_comments= df[df['KMeans_Cluster']==cluster]['Cleaned_Comment'] # df['KMeans_Cluster]==cluster creates a series of True and False which checks if the each row in Cluster matches the cluster. df[df['Cluster']==cluster] will return new dataframe which will only contains the rows where the Cluster column matches particular cluster. [cleaned_Comment] with the others will contain only the cleaned comments that will belong to a particualr cluster\n",
        "  text=' '.join(cluster_comments) # the comments that are similar will form a string\n",
        "  wordcloud=WordCloud(width=800, height=400).generate(text) # .generate(text) is used as it takes the text data and than gives it to the wordcloud for the visual representation\n",
        "  plt.imshow(wordcloud, interpolation='bilinear') # interpolation is used to define the color of th pixels. billinear uses closest 2X2 matrix. it first perfoms the linear and than other directions. this results the smoother image as comparision to the naerest neighbor interpolation\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(f\"KMeans Cluster {cluster}\")\n",
        "  plt.show()\n",
        "  # this helped us to see the most common words in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juE2andPZYsE",
        "outputId": "5b502e78-ebe9-4898-ac6a-faefbe6461d0"
      },
      "outputs": [],
      "source": [
        "# we gotta identify the event in the clusters\n",
        "for cluster in range (number_of_clusters):\n",
        "  cluster_comments=df[df['KMeans_Cluster']==cluster]['Cleaned_Comment']\n",
        "  all_words=' '.join(cluster_comments).split() # taking the cluster_comments and and join them in a single string with space. from here the split splits the string at whitespaces, tabs, newlines and return a list of words\n",
        "  common_words=Counter(all_words).most_common(10) # 10 is to get the most common 10 words\n",
        "  print(f\"Cluster {cluster} top words: {common_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dbjx4kCibrdJ",
        "outputId": "15956199-5eb1-40b1-f93f-990a089fb603"
      },
      "outputs": [],
      "source": [
        "# we will detect the event\n",
        "event_keywords=[\"earthquake\", \"protest\", \"accident\", \"breaking\", \"viral\", \"trending\"]\n",
        "def detect_events(cluster_comments):\n",
        "  for keyword in event_keywords:\n",
        "    if keyword in cluster_comments:\n",
        "      return True\n",
        "  return False\n",
        "df['is event']=df['Cleaned_Comment'].apply(detect_events)\n",
        "print(df[df['is event'] == True]) # printing the detected event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ6IQwq1d2hS",
        "outputId": "56f90ffe-a77f-4733-cd24-208319e161e2"
      },
      "outputs": [],
      "source": [
        "# to see hpow well the clustering is\n",
        "from sklearn.metrics import silhouette_score # if the score is near 1 than clustering is good. if its near than it is bad\n",
        "silhouette_avg= silhouette_score(x, df[\"KMeans_Cluster\"])\n",
        "print(f\"silhouette score:{silhouette_avg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "GCJkgXMAfImr",
        "outputId": "160e73da-f411-4af8-8f85-63851d11ae76"
      },
      "outputs": [],
      "source": [
        "# elbow mwthod ( for checking how many clusters should be made)\n",
        "inertia=[]\n",
        "K=range(1,11)\n",
        "for k in K:\n",
        "  kmeans=KMeans(n_clusters=k,random_state=42)\n",
        "  kmeans.fit(x)\n",
        "  inertia.append(kmeans.inertia_)\n",
        "plt.plot(K, inertia, 'bx-')\n",
        "plt.xlabel(\"number of clusters\")\n",
        "plt.ylabel(\"inertia\")\n",
        "plt.title(\"elbow method\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YttyuEm1gjU8",
        "outputId": "41153aa9-f198-44a1-82f8-13da1473938e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score # if the score is less it means good\n",
        "db_score=davies_bouldin_score(x.toarray(), df[\"KMeans_Cluster\"])\n",
        "print(f\"davies bouldin index: {db_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "txjRaEpPgEWX",
        "outputId": "9181fbf1-c826-40a7-d071-a20599ef8118"
      },
      "outputs": [],
      "source": [
        "# visual inspection to see how far the clusters are: (if the silhouette score is less yet the number of cluster needed is good than one of the reasons could be that distance between the clusters colud be less)\n",
        "# PCA (principal component analysis) ot t-SNE (t-distribution stochastic neighbor embedding) is used to see how well- separated clusters are\n",
        "from sklearn.decomposition import PCA\n",
        "pca=PCA(n_components=2)\n",
        "reduced_data=pca.fit_transform(x.toarray())\n",
        "plt.scatter(reduced_data[:,0], reduced_data[:, 1], c=df[\"KMeans_Cluster\"], cmap=\"viridis\", marker=\"o\")\n",
        "plt.title(\"pca clusters\")\n",
        "plt.xlabel(\"pca 1\")\n",
        "plt.ylabel(\"pca 2\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
